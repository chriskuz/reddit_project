{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitdsienvcondac0f786d7361b46f9a35a24c21f171f2b",
   "display_name": "Python 3.7.6 64-bit ('DSI_env': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping\n",
    "\n",
    "Thanks to Reddit's semi-relaxed platform and conditions of data scraping, [sourced here in their robots.txt path](https://www.reddit.com/robots.txt), we are able to successfully obtain various data to help us create a model that answers our problem statement. While there is no actual subreddit known as \"r/dangerouslycuteanimals\" to actually obtain the jumbled data which my girlfriend is complaining about, we are able to simulate this dataframe conundrum by pulling data from the two original reddits whose content were merged together: \"r/natureismetal\" and \"r/aww.\" Upon pulling both sets of data form the reddits, we can then combine them to create one large dataframe to work with.  \n",
    "\n",
    "The most easy way of doing this without needing to go through an arduous web-scraping process of navigating ugly HTML is by utilizing an API. APIs are tools made by website developers and/or data-enthusiastic communities which help users (such as ourselves) access features to software. Some APIs are sophisticated enough to allow full modding support for certain software applications. In this case, we are going to use a web API known as [pushshift](https://pushshift.io) to obtain information from Reddit's different communities. Pushshift is a [community generated API originally made by moderators from \"r/datasets\"](https://github.com/pushshift/api). Its documentation is found in both related pushshift hyperlink sources found within this very paragraph. It is very sophisticated and very useful in quickly obtaining data in json key-value pairs which can be readily parsed through in Python.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #imports pandas package\n",
    "import datetime as dt #imports datetime package\n",
    "import time #imports time package\n",
    "import requests #imports requests package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API has the capability of pulling in data from two main pathways. One pathway searches through reddit submissions (mainly including posts) while the other pathway searches through comments. For this study, we are only going to go through Reddit submissions to help us gather our data for our model. The Reddit submission pathway through the API contains a lot of other accessible data, including the author name of the post, popularity of the post as whole, and more. As a frequent Reddit user and commentater, I personally can vouch for why it may not be wise to use comments as a method for creating a model. The general understanding to remember is this: *posts on reddit go through bot moderated scruitiny, whereas comments do not*. To elaborate, each successful subreddit will have strict guidelines and rules for submitting content relevant to the \"sub\" and will often have a very dedicated team of moderators who oversee that these rules are followed to the best of everyone's abilities. These rules allow the majority of submitted content to stay relevant to the community's interests and stay relevant to the subreddit's purpose. Comments are typically more plentiful than submissions and are often overlooked by moderators. Comments as an entity are always left open to interpretation and do not undergo as much scrutiny as posts do. This allows commentators to write narratives that may go on complete tangents from the original context of the submission -- solely in the spirit of online discussion. As a reuslt, we may find examples of people talking about their favorite movies or foods on subreddits only discussing how cute cats are.\n",
    "\n",
    "The next step is to consider which features are going to be important to understand for our model. Using the API's documentation, we can decide on these "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit to Mahdi Shadkam-Farrokhi for fundtion\n",
    "#The below function obtains and \"cleans\" the data from a subreddit. \n",
    "#The below function utilizes the pushshift API\n",
    "\n",
    "def query_pushshift(subreddit, kind = 'submission', day_window = 30, n = 5):\n",
    "    SUBFIELDS = ['title', 'selftext', 'subreddit', 'created_utc', 'author', 'num_comments', 'score', 'is_self', 'over_18', 'author_flair_text', 'total_awards_received']\n",
    "    \n",
    "    # establish base url and stem\n",
    "    BASE_URL = f\"https://api.pushshift.io/reddit/search/{kind}\" # also known as the \"API endpoint\" \n",
    "    stem = f\"{BASE_URL}?subreddit={subreddit}&size=500\" # always pulling max of 500\n",
    "    \n",
    "    # instantiate empty list for temp storage\n",
    "    posts = []\n",
    "    \n",
    "    # implement for loop with `time.sleep(2)`\n",
    "    for i in range(1, n + 1):\n",
    "        URL = \"{}&after={}d\".format(stem, day_window * i)\n",
    "        print(\"Querying from: \" + URL)\n",
    "        response = requests.get(URL)\n",
    "        assert response.status_code == 200\n",
    "        mine = response.json()['data']\n",
    "        df = pd.DataFrame.from_dict(mine)\n",
    "        posts.append(df)\n",
    "        time.sleep(5)\n",
    "    \n",
    "    # pd.concat storage list\n",
    "    full = pd.concat(posts, sort=False)\n",
    "    \n",
    "    # if submission\n",
    "    if kind == \"submission\":\n",
    "        # select desired columns\n",
    "        full = full[SUBFIELDS]\n",
    "        # drop duplicates\n",
    "        full.drop_duplicates(inplace = True)\n",
    "        # select `is_self` == True\n",
    "        #full = full.loc[full['is_self'] == True]\n",
    "\n",
    "    # create `timestamp` column\n",
    "    full['timestamp'] = full[\"created_utc\"].map(dt.date.fromtimestamp)\n",
    "\n",
    "\n",
    "    full.reset_index(inplace = True)\n",
    "    print(\"Query Complete!\")    \n",
    "    return full "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=natureismetal&size=500&after=30d\nQuerying from: https://api.pushshift.io/reddit/search/submission?subreddit=natureismetal&size=500&after=60d\nQuerying from: https://api.pushshift.io/reddit/search/submission?subreddit=natureismetal&size=500&after=90d\nQuerying from: https://api.pushshift.io/reddit/search/submission?subreddit=natureismetal&size=500&after=120d\nQuerying from: https://api.pushshift.io/reddit/search/submission?subreddit=natureismetal&size=500&after=150d\nQuery Complete!\n"
    }
   ],
   "source": [
    "nature_is_metal = query_pushshift(\"natureismetal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aww = query_pushshift(\"aww\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Jaguar killing big Caiman in water'"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nature_is_metal.loc[2, 'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['author', 'domain', 'full_link', 'is_self', 'num_comments','over_18', 'selftext', 'subreddit_type','subreddit', 'total_awards_received', 'created_utc']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>all_awardings</th>\n      <th>associated_award</th>\n      <th>author</th>\n      <th>author_flair_background_color</th>\n      <th>author_flair_css_class</th>\n      <th>author_flair_richtext</th>\n      <th>author_flair_template_id</th>\n      <th>author_flair_text</th>\n      <th>author_flair_text_color</th>\n      <th>...</th>\n      <th>send_replies</th>\n      <th>stickied</th>\n      <th>subreddit</th>\n      <th>subreddit_id</th>\n      <th>total_awards_received</th>\n      <th>author_cakeday</th>\n      <th>distinguished</th>\n      <th>steward_reports</th>\n      <th>edited</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>[]</td>\n      <td>None</td>\n      <td>dyllmatic777</td>\n      <td>None</td>\n      <td>None</td>\n      <td>[]</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>True</td>\n      <td>False</td>\n      <td>natureismetal</td>\n      <td>t5_324zi</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2020-03-21</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>[]</td>\n      <td>None</td>\n      <td>reddatazz</td>\n      <td>None</td>\n      <td>None</td>\n      <td>[]</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>True</td>\n      <td>False</td>\n      <td>natureismetal</td>\n      <td>t5_324zi</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2020-03-21</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>[]</td>\n      <td>None</td>\n      <td>Titaniumspyborgbear</td>\n      <td>None</td>\n      <td>None</td>\n      <td>[]</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>True</td>\n      <td>False</td>\n      <td>natureismetal</td>\n      <td>t5_324zi</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2020-03-21</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>[]</td>\n      <td>None</td>\n      <td>sm1rr0r</td>\n      <td>None</td>\n      <td>None</td>\n      <td>[]</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>True</td>\n      <td>False</td>\n      <td>natureismetal</td>\n      <td>t5_324zi</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2020-03-21</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>[]</td>\n      <td>None</td>\n      <td>Shadowbanned_User</td>\n      <td>None</td>\n      <td>None</td>\n      <td>[]</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>True</td>\n      <td>False</td>\n      <td>natureismetal</td>\n      <td>t5_324zi</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2020-03-21</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 38 columns</p>\n</div>",
      "text/plain": "   index all_awardings associated_award               author  \\\n0      0            []             None         dyllmatic777   \n1      1            []             None            reddatazz   \n2      2            []             None  Titaniumspyborgbear   \n3      3            []             None              sm1rr0r   \n4      4            []             None    Shadowbanned_User   \n\n  author_flair_background_color author_flair_css_class author_flair_richtext  \\\n0                          None                   None                    []   \n1                          None                   None                    []   \n2                          None                   None                    []   \n3                          None                   None                    []   \n4                          None                   None                    []   \n\n  author_flair_template_id author_flair_text author_flair_text_color  ...  \\\n0                     None              None                    None  ...   \n1                     None              None                    None  ...   \n2                     None              None                    None  ...   \n3                     None              None                    None  ...   \n4                     None              None                    None  ...   \n\n  send_replies stickied      subreddit subreddit_id total_awards_received  \\\n0         True    False  natureismetal     t5_324zi                     0   \n1         True    False  natureismetal     t5_324zi                     0   \n2         True    False  natureismetal     t5_324zi                     0   \n3         True    False  natureismetal     t5_324zi                     0   \n4         True    False  natureismetal     t5_324zi                     0   \n\n  author_cakeday distinguished  steward_reports edited   timestamp  \n0            NaN           NaN              NaN    NaN  2020-03-21  \n1            NaN           NaN              NaN    NaN  2020-03-21  \n2            NaN           NaN              NaN    NaN  2020-03-21  \n3            NaN           NaN              NaN    NaN  2020-03-21  \n4            NaN           NaN              NaN    NaN  2020-03-21  \n\n[5 rows x 38 columns]"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.reset_index(inplace = True)\n",
    "print(type(full)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "domain, full_link, is_original_content, is_self, link_flair_background_color, link_flair_text_color, link_flair_type, num_comments, over_18, selftext, subreddit_type, title, url, crosspost_parent, crosspost_parent_list, link_flair_css_class, link_flair_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(0, 9)"
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nature_is_metal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2    []\n2    []\n2    []\n2    []\n2    []\nName: all_awardings, dtype: object"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nature_is_metal.loc[2,'all_awardings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}