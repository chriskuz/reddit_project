{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitdsienvcondac0f786d7361b46f9a35a24c21f171f2b",
   "display_name": "Python 3.7.6 64-bit ('DSI_env': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping\n",
    "\n",
    "Thanks to Reddit's semi-relaxed platform and conditions of data scraping, [sourced here in their robots.txt path](https://www.reddit.com/robots.txt), we are able to successfully obtain various data to help us create a model that answers our problem statement. While there is no actual subreddit known as \"r/dangerouslycute\" to actually obtain the jumbled data which my girlfriend is complaining about, we are able to simulate this dataframe conundrum by pulling data from the two original reddits whose content were merged together: \"r/natureismetal\" and \"r/aww.\" Upon pulling both sets of data form the reddits, we can then combine them to create one large dataframe to work with.  \n",
    "\n",
    "The most easy way of doing this without needing to go through an arduous web-scraping process of navigating ugly HTML is by utilizing an API. APIs are tools made by website developers and/or data-enthusiastic communities which help users (such as ourselves) access features to software. Some APIs are sophisticated enough to allow full modding support for certain software applications. In this case, we are going to use a web API known as [pushshift](https://pushshift.io) to obtain information from Reddit's different communities. Pushshift is a [community generated API originally made by moderators from \"r/datasets\"](https://github.com/pushshift/api). Its documentation is found in both related pushshift hyperlink sources found within this very paragraph. It is very sophisticated and very useful in quickly obtaining data in json key-value pairs which can be readily parsed through in Python.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #imports pandas package\n",
    "import datetime as dt #imports datetime package\n",
    "import time #imports time package\n",
    "import requests #imports requests package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Access Do We Have to the Data & What Data Do We Have Access to?\n",
    "\n",
    "The API has the capability of pulling in data from two main pathways. One pathway searches through reddit submissions (mainly including posts) while the other pathway searches through comments. For this study, we are only going to go through Reddit submissions to help us gather our data for our model. The Reddit submission pathway through the API contains a lot of other accessible data, including the author name of the post, popularity of the post as whole, and more. As a frequent Reddit user and commentater, I personally can vouch for why it may not be wise to use comments as a method for creating a model. The general understanding to remember is this: *posts on reddit go through bot moderated scruitiny, whereas comments do not*. To elaborate, each successful subreddit will have strict guidelines and rules for submitting content relevant to the \"sub\" and will often have a very dedicated team of moderators who oversee that these rules are followed to the best of everyone's abilities. These rules allow the majority of submitted content to stay relevant to the community's interests and stay relevant to the subreddit's purpose. Comments are typically more plentiful than submissions and are often overlooked by moderators. Comments as an entity are always left open to interpretation and do not undergo as much scrutiny as posts do. This allows commentators to write narratives that may go on complete tangents from the original context of the submission -- solely in the spirit of online discussion. As a reuslt, we may find examples of people talking about their favorite movies or foods on subreddits only discussing how cute cats are.\n",
    "\n",
    "The next step is to consider which features are going to be important to understand for our model. Using the API's documentation, we can decide on these features right here to make any necessary data cleanup later on easier. The list of features which were thought to be relevant for our model are:\n",
    "\n",
    "- `title` (title of the submission)\n",
    "- `selftext` (text of post submission )\n",
    "-  `subreddit` (name of the submission's associated subreddit)\n",
    "- `created_utc` (time stamp of submission)\n",
    "- `author` (name of the submission author)\n",
    "- `num_comments` (number of comments with the submission)\n",
    "- `score` (aggregated score of the submission incorporating the difference of upvotes and downvotes)\n",
    "-  `is_self` (boolean to determine if the submission is solely text post)\n",
    "-  `over_18` (boolean to determine if content is NSFW)\n",
    "- `author_flair_text` (flair text native to the author when posting on a specific subreddit)\n",
    "- `total_awards_received` (number of awards received)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why These Features?\n",
    "\n",
    "These features were thought out and chosen out of the plethora of features we could pull because of their possible predictive ability to create our model. Below is a quick list of reasoning behind each variable's selection:\n",
    "\n",
    "- `title` (necessary to determine titular key words)\n",
    "- `selftext` (useful to find any key words to be added to our NLP modeling)\n",
    "-  `subreddit` (our prediction values)\n",
    "- `created_utc` (verifies the uniqueness of data)\n",
    "- `author` (may be useful in determining user interest per subreddit)\n",
    "- `num_comments` (useful to find popular submissions within a subreddit)\n",
    "- `score` (useful in determining the validity of a post to a subreddit by its popularity)\n",
    "-  `is_self` (useful in identifying posts with added text)\n",
    "-  `over_18` (boolean to determine if content is NSFW)\n",
    "- `author_flair_text` (may be useful is finding relevant text related to a subreddit per author)\n",
    "- `total_awards_received` (useful in understanding a post's weight on the subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pushshift API Custom Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit to Mahdi Shadkam-Farrokhi for fundtion\n",
    "#The below function obtains and \"cleans\" the data from a subreddit. \n",
    "#The below function utilizes the pushshift API\n",
    "\n",
    "def query_pushshift(subreddit, kind = 'submission', day_window = 30, n = 5):\n",
    "    SUBFIELDS = ['title', 'selftext', 'subreddit', 'created_utc', 'author', 'num_comments', 'score', 'is_self', 'over_18', 'author_flair_text', 'total_awards_received'] #relevant subfields\n",
    "    \n",
    "    # establish base url and stem\n",
    "    BASE_URL = f\"https://api.pushshift.io/reddit/search/{kind}\" # also known as the \"API endpoint\" \n",
    "    stem = f\"{BASE_URL}?subreddit={subreddit}&size=500\" # always pulling max of 500\n",
    "    \n",
    "    # instantiate empty list for temp storage\n",
    "    posts = []\n",
    "    \n",
    "    # implement for loop with `time.sleep(2)`\n",
    "    for i in range(1, n + 1):\n",
    "        URL = \"{}&after={}d\".format(stem, day_window * i) #calls the URL we are searching based on function input\n",
    "        print(\"Querying from: \" + URL) #displays the reddit and path we are querying from\n",
    "        response = requests.get(URL) #grabs the actual data\n",
    "        assert response.status_code == 200 #will give us an error if the request is not met\n",
    "        mine = response.json()['data'] #grabs the json file of the relevant data\n",
    "        df = pd.DataFrame.from_dict(mine) #converts the json json data into a dataframe\n",
    "        posts.append(df) #appends this dataframe into a list known as posts\n",
    "        time.sleep(5) #sets a sleep timer for 5 seconds to \n",
    "    \n",
    "    # pd.concat storage list\n",
    "    full = pd.concat(posts, sort=False) #concats the list of dataframes into a giant dataframe\n",
    "    \n",
    "    # if submission\n",
    "    if kind == \"submission\":\n",
    "        # select desired columns\n",
    "        full = full[SUBFIELDS] #putting in our subfields\n",
    "        # drop duplicates\n",
    "        full.drop_duplicates(inplace = True) #drops duplicates in our giant dataframe \n",
    "\n",
    "    # create `timestamp` column\n",
    "    full['timestamp'] = full[\"created_utc\"].map(dt.date.fromtimestamp) #converts the utc column into proper date time\n",
    "\n",
    "\n",
    "    full.reset_index(inplace = True) #resets the index to eliminate the index repetition\n",
    "    print(\"Query Complete!\") #lets us know when the query is complete    \n",
    "    return full "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=natureismetal&size=500&after=30d\nQuerying from: https://api.pushshift.io/reddit/search/submission?subreddit=natureismetal&size=500&after=60d\nQuerying from: https://api.pushshift.io/reddit/search/submission?subreddit=natureismetal&size=500&after=90d\nQuerying from: https://api.pushshift.io/reddit/search/submission?subreddit=natureismetal&size=500&after=120d\nQuerying from: https://api.pushshift.io/reddit/search/submission?subreddit=natureismetal&size=500&after=150d\nQuery Complete!\n"
    }
   ],
   "source": [
    "nature_is_metal = query_pushshift(\"natureismetal\") #pulls from the subreddit r/natureismetal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=aww&size=500&after=30d\nQuerying from: https://api.pushshift.io/reddit/search/submission?subreddit=aww&size=500&after=60d\nQuerying from: https://api.pushshift.io/reddit/search/submission?subreddit=aww&size=500&after=90d\nQuerying from: https://api.pushshift.io/reddit/search/submission?subreddit=aww&size=500&after=120d\nQuerying from: https://api.pushshift.io/reddit/search/submission?subreddit=aww&size=500&after=150d\nQuery Complete!\n"
    }
   ],
   "source": [
    "aww = query_pushshift(\"aww\") #pulls from the subreddit r/aww"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*All sources better referenced in the  `project_3_main.ipynb` file under the section \"Sources and References\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}